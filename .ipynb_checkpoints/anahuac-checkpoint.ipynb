{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MARATONA BEHIND THE CODE 2020\n",
    "\n",
    "## DESAFIO 6 - ANAHUAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este desafio, usted usará herramientas de IBM como Watson Studio (o Cloud Pak for Data) para construir un modelo baseado en Machine Learning capaz de preveer si un estudante irá continuar o abandonará su curso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the .csv dataset from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/datasets/ForTraining.csv\n",
    "df_base_for_training = pd.read_csv(r'ForTraining.csv')\n",
    "df_base_for_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción: La primera tabla mostrada arriba tiene 4 columnas, 3 son features and el target: `Graduado` that has a binary values={Si, No}.\n",
    "\n",
    "Usted puede, y debe, usar mas data que esta disponible para construir su modelo. Los siguientes archivos .csv presentados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/datasets/OrdenMaterias.csv\n",
    "df_orden_materias = pd.read_csv(r'OrdenMaterias.csv')\n",
    "df_orden_materias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/datasets/TablaConexiones.csv\n",
    "df_tabla_conexiones = pd.read_csv(r'TablaConexiones.csv')\n",
    "df_tabla_conexiones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/datasets/TablaTareas.csv\n",
    "df_tabla_tareas = pd.read_csv(r'TablaTareas.csv')\n",
    "df_tabla_tareas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview del Dataset:\n",
    "\n",
    "    Disponibles para el participante, ecisten 4 tables cargas en DataFrames anteriormente:\n",
    "    \n",
    "    **df_base_for_training**\n",
    "        - ``studentId``\n",
    "        ``reducido``\n",
    "        ``ciclo``\n",
    "        ``Graduado`` --> ¡LA VARIABLE OBJETIVO PARA CLASIFICACIÓN BINARIA!\n",
    "        \n",
    "    **df_orden_materias**\n",
    "        ``reducido``\n",
    "        ``2017 - 03``\n",
    "        ``2017 - 04``\n",
    "        ``2017 - 05``\n",
    "        ``2017 - 06``\n",
    "        ``2017 - 07``\n",
    "        ``2017 - 08``\n",
    "        ``2018 - 01``\n",
    "        ``2018 - 02``\n",
    "        ``2018 - 03``\n",
    "        ``2018 - 04``\n",
    "        ``2018 - 05``\n",
    "        ``2018 - 06``\n",
    "        ``2018 - 07``\n",
    "        ``2018 - 08``\n",
    "        ``2019 - 01``\n",
    "        ``2019 - 02``\n",
    "        ``2019 - 03``\n",
    "        ``2019 - 04``\n",
    "        ``2019 - 05``\n",
    "        ``2019 - 06``\n",
    "        ``2019 - 07``\n",
    "        ``2019 - 08``\n",
    "        ``2020 - 01``\n",
    "        ``2020 - 02``\n",
    "        ``2020 - 03``\n",
    "        ``2020 - 04``\n",
    "        ``2020 - 05``\n",
    "        ``2020 - 06``\n",
    "        \n",
    "    **df_tabla_conexiones**\n",
    "        - ``studentId``\n",
    "        ``ciclo``\n",
    "        ``Dias_Conectado``\n",
    "        ``Minutos_Promedio``\n",
    "        ``Minutos_Total``\n",
    "        \n",
    "    **df_tabla_tareas**\n",
    "        - ``studentId``\n",
    "        ``ciclo``\n",
    "        ``Calificacion_Promedio``\n",
    "        ``Tareas_Puntuales``\n",
    "        ``Tareas_No_Entregadas``\n",
    "        ``Tareas_Retrasadas``\n",
    "        ``Total_Tareas``\n",
    "        \n",
    "Observe que la variable ``studentId`` aparece en varias tablas.\n",
    "\n",
    "Usted puede combinar/merge estos datasets como usted desee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columnas en *df_base_for_training*:\")\n",
    "print(df_base_for_training.columns)\n",
    "\n",
    "print(\"\\Columnas en *df_orden_materias*:\")\n",
    "print(df_orden_materias.columns)\n",
    "\n",
    "print(\"\\Columnas en *df_tabla_conexiones*:\")\n",
    "print(df_tabla_conexiones.columns)\n",
    "\n",
    "print(\"\\Columnas en *df_tabla_tareas*:\")\n",
    "print(df_tabla_tareas.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¡ATENCIÓN! La columna **target** es  ``Graduado``, presente en el DataFrame \"df_base_for_training\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniendo DataFrames en Pandas\n",
    "\n",
    "Documentación oficial para Pandas 1.1.0: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como un **ejemplo** de como usar Pandas, camos a unir/merge la información de las tablas \"df_base_for_training\" y \"df_tabla_tareas\" a traves de la llave ``studentId``.\n",
    "\n",
    "Usted puee editar el dataframes manualmente si lo prefiere, usando Microsoft Excel u otros lenguajes. Recuerde insertar la data procesada en IBM Cloud Pak for Data para que pueda entrenar su modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_for_training.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tabla_tareas.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El resultado de esta celda sera la union de los dos anteriores dataframes\n",
    "# usando la columna ``studentId`` como llave.\n",
    "\n",
    "df = pd.merge(\n",
    "    df_base_for_training, df_tabla_tareas, how='inner',\n",
    "    on=None, left_on=['studentId', 'ciclo'], right_on=['studentId', 'ciclo'],\n",
    "    left_index=False, right_index=False, sort=True,\n",
    "    suffixes=('_x', '_y'), copy=True, indicator=False,\n",
    "    validate=None\n",
    ")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información acerca de las columnas del dataset unido\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la información de arriba ud puede observar que hay valores Null/NaN en algunas de las columnas.\n",
    "\n",
    "Para que nuestro modelo quede bien entrenado necesitamos procesar estos valores nulos de una forma adecuada.\n",
    "\n",
    "Usted escogera la mejor estrategia como parte del desafío, pero en la siguiente celda encuentra un **ejemplo** the como puede hacer este procesamiento usanto la libreria *scikit-learn*.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesando el dataset antes de entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borrando finlas con valores NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el metodo Pandas DataFrame.dropna() usted puede remover todas las filas que estan indefinidas para la columna ``Graduado``.\n",
    "\n",
    "Docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando los datos faltantes del dataset antes de la primera transformación (df_data_2)\n",
    "print(\"Valores nulos antes de la transformación DropNA: \\n\\n{}\\n\".format(df.isnull().sum(axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando la función para borrar todas las filas con valor NaN en la columna ``Graduado``:\n",
    "df2 = df.dropna(axis='index', how='any', subset=['Graduado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando los datos faltantes del dataset antes de la primera transformación (SimpleImputer) (df_data_3)\n",
    "print(\"Valores nulos antes de la transformación SimpleImputer: \\n\\n{}\\n\".format(df2.isnull().sum(axis = 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesando valores NaN con SimpleImputer de sklearn\n",
    "\n",
    "Para los otros valores NaN vamos a usar como **ejemplo** la sustitución por la constante 0. \n",
    "\n",
    "Usted puede escoger cualquier estrategia que crea que es la mejor para esto :)\n",
    "\n",
    "Docs: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html?highlight=simpleimputer#sklearn.impute.SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Creando un objeto ``SimpleImputer``\n",
    "impute_zeros = SimpleImputer(\n",
    "    missing_values=np.nan,  # Los valores faltantes son de tipo ``np.nan`` (estandar Pandas)\n",
    "    strategy='constant',  # La estrategia escogida es reemplazar por una constante\n",
    "    fill_value=0,  # La constante que será usada para reemmplazar los valores faltantes es un int64=0.\n",
    "    verbose=0,\n",
    "    copy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando los datos faltantes del dataset antes de la segunda transformación (df_data_2)\n",
    "print(\"Valores nulos antes de transformación SimpleImputer: \\n\\n{}\\n\".format(df2.isnull().sum(axis = 0)))\n",
    "\n",
    "# Aplicar la transformación ``SimpleImputer`` en el conjunto de datos base\n",
    "impute_zeros.fit(X=df)\n",
    "\n",
    "# Reconstrucción del nuevo DataFrame Pandas (df_data_3)\n",
    "df = pd.DataFrame.from_records(\n",
    "    data=impute_zeros.transform(\n",
    "        X=df\n",
    "    ),  # El resultado SimpleImputer.transform(<<pandas dataframe>>) es una lista de listas\n",
    "    columns=df.columns  # Las columnas originals deben ser conservadas en esta transformación\n",
    ")\n",
    "\n",
    "# Visualizndo los datos faltantes del dataset \n",
    "print(\"Valores nulos del dataset despues de la transformación SimpleImputer: \\n\\n{}\\n\".format(df.isnull().sum(axis = 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminando columnas no desadas\n",
    "\n",
    "Vamos a **demostrar** acontinuación como usar el metodo DataFrame.drop().\n",
    "\n",
    "Docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(columns=['reducido'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejando variables Categoricas\n",
    "\n",
    "Como se menciono antes, los computadores no son buenos con las variables categoricas.\n",
    "\n",
    "Mientras que nosotros entendemos bien las variables categoricas, es debido a un conocimiento previo quie el computador no tiene.\n",
    "\n",
    "La mayoria de tecnicas de Machine Learning y modelso trabajan con un set limitado de datos (Tipicamente binario). \n",
    "\n",
    "Las redes neurales consumenda data y producen resultados en el rango de 0..1 t raramente van mas alla del alcance.\n",
    "\n",
    "En resumen, la gran mayoria de algoritmos de machine learning aceptan data de entrada  (\"training data\") de donde los features son extraidos.\n",
    "\n",
    "Basado en estos features, un modelo matematico es creado, el cual es usado para hacer una predicción o decision sin ser programado explicitamente para esa tarea.\n",
    "\n",
    "Dado un dataset con con 2 features, vamos a dejar que encoder encuentre los valores unicos por features y transforme la data a binario usando la tecnica one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas One-hot-encoding del dataset usando el metodo de Pandas ``get_dummies``  (demontración)\n",
    "df3 = pd.get_dummies(df2, columns=['ciclo'])\n",
    "df3.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando un clasificador basado  en un Árbol de Decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionando FEATURES y definiendo la variable TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df3[\n",
    "    [\n",
    "        'studentId', 'Calificacion_Promedio', 'Tareas_Puntuales',\n",
    "        'Tareas_No_Entregadas', 'Tareas_Retrasadas', 'Total_Tareas',\n",
    "        'ciclo_2017 - 03', 'ciclo_2017 - 04', 'ciclo_2017 - 05',\n",
    "        'ciclo_2017 - 06', 'ciclo_2017 - 07', 'ciclo_2017 - 08',\n",
    "        'ciclo_2018 - 01', 'ciclo_2018 - 02', 'ciclo_2018 - 03',\n",
    "        'ciclo_2018 - 04', 'ciclo_2018 - 05', 'ciclo_2018 - 06',\n",
    "        'ciclo_2018 - 07', 'ciclo_2018 - 08', 'ciclo_2019 - 01',\n",
    "        'ciclo_2019 - 02', 'ciclo_2019 - 03', 'ciclo_2019 - 04',\n",
    "        'ciclo_2019 - 05', 'ciclo_2019 - 06', 'ciclo_2019 - 07',\n",
    "        'ciclo_2019 - 08'\n",
    "    ]\n",
    "]\n",
    "target = df3['Graduado']  ## No cambie la variable target!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividiendo nuestro dataset en set de Entrenamiento y Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=133)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando un modelo ``DecisionTreeClassifier()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método para creacion de modelos basados en arbol de desición\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=15).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haciendo predicciones del Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dtc.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analice la calidad del modelo a través de la matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "group_names = ['`Positivo` Correto', '`Negativo` Errado', 'Falso `Positivo`', '`Negativo` Correto']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "accuracy  = np.trace(cf_matrix) / float(np.sum(cf_matrix))\n",
    "precision = cf_matrix[1,1] / sum(cf_matrix[:,1])\n",
    "recall    = cf_matrix[1,1] / sum(cf_matrix[1,:])\n",
    "f1_score  = 2*precision*recall / (precision + recall)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt=\"\")\n",
    "stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={}\".format(accuracy, precision, recall, f1_score)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label' + stats_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring de la data requerida para hacer la entrega de la solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el envío, necesita clasificar el siguiente dataset. Para hacer eso, usted necesita reproducir los mismos pasos de pre-procesamiento para que el dataset este en la misma estructura del que usted uso para construir su modelo. Despues de clasificar este dataframe, esperamos que usted entregue un archivo csv con las 2499 filar y una columna 'Graduado' con su predicción. **No cambie el orden del archivo a predecir ni borre filas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/for_submission/ToBePredicted.csv\n",
    "df_to_be_predicted = pd.read_csv(r'ToBePredicted.csv')\n",
    "df_to_be_predicted.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniendo los dataset\n",
    "df = pd.merge(\n",
    "    df_to_be_predicted, df_tabla_tareas, how='inner',\n",
    "    on=None, left_on=['studentId', 'ciclo'], right_on=['studentId', 'ciclo'],\n",
    "    left_index=False, right_index=False, sort=True,\n",
    "    suffixes=('_x', '_y'), copy=True, indicator=False,\n",
    "    validate=None\n",
    ")\n",
    "\n",
    "# Eliminando la columna 'reducido'\n",
    "df2 = df.drop(columns=['reducido'], inplace=False)\n",
    "\n",
    "# Columnas One-hot-encoding del dataset usando el metodo de Pandas ``get_dummies``  (demontración)\n",
    "df3 = pd.get_dummies(df2, columns=['ciclo'])\n",
    "df3.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando los features declarados acontinuación, sabemos que el dataset ha ser evaluado esta en el mismo formato usado para entrenar nuestro árbol de decisión anteriormente.\n",
    "\n",
    "```features = df3[\n",
    "    [\n",
    "        'studentId', 'Calificacion_Promedio', 'Tareas_Puntuales',\n",
    "        'Tareas_No_Entregadas', 'Tareas_Retrasadas', 'Total_Tareas',\n",
    "        'ciclo_2017 - 03', 'ciclo_2017 - 04', 'ciclo_2017 - 05',\n",
    "        'ciclo_2017 - 06', 'ciclo_2017 - 07', 'ciclo_2017 - 08',\n",
    "        'ciclo_2018 - 01', 'ciclo_2018 - 02', 'ciclo_2018 - 03',\n",
    "        'ciclo_2018 - 04', 'ciclo_2018 - 05', 'ciclo_2018 - 06',\n",
    "        'ciclo_2018 - 07', 'ciclo_2018 - 08', 'ciclo_2019 - 01',\n",
    "        'ciclo_2019 - 02', 'ciclo_2019 - 03', 'ciclo_2019 - 04',\n",
    "        'ciclo_2019 - 05', 'ciclo_2019 - 06', 'ciclo_2019 - 07',\n",
    "        'ciclo_2019 - 08'\n",
    "    ]\n",
    "]\n",
    "target = df3['Graduado']  ## No cambie la variable target!```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dtc.predict(df3[\n",
    "    [\n",
    "        'studentId', 'Calificacion_Promedio', 'Tareas_Puntuales',\n",
    "        'Tareas_No_Entregadas', 'Tareas_Retrasadas', 'Total_Tareas',\n",
    "        'ciclo_2017 - 03', 'ciclo_2017 - 04', 'ciclo_2017 - 05',\n",
    "        'ciclo_2017 - 06', 'ciclo_2017 - 07', 'ciclo_2017 - 08',\n",
    "        'ciclo_2018 - 01', 'ciclo_2018 - 02', 'ciclo_2018 - 03',\n",
    "        'ciclo_2018 - 04', 'ciclo_2018 - 05', 'ciclo_2018 - 06',\n",
    "        'ciclo_2018 - 07', 'ciclo_2018 - 08', 'ciclo_2019 - 01',\n",
    "        'ciclo_2019 - 02', 'ciclo_2019 - 03', 'ciclo_2019 - 04',\n",
    "        'ciclo_2019 - 05', 'ciclo_2019 - 06', 'ciclo_2019 - 07',\n",
    "        'ciclo_2019 - 08'\n",
    "    ]\n",
    "])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando los resultados de la predicción en un archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"results.csv\", y_pred, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save_data(file_name=\"results.csv\", data=pd.read_csv(\"results.csv\", header=None).to_csv(header=[\"TARGET\"], index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## ¡Felicitaciones!\n",
    "\n",
    "Si todo fue ejecutado sin errores, usted ya tiene un modelo basado en classificacion binaria y puede descargar sus resultados para subirlos como csv!\n",
    "\n",
    "Para enviar su solución, ve a la página:\n",
    "\n",
    "# https://anahuac.maratona.dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
